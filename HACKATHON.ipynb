{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumathi3399/hackathon_ai_model/blob/main/HACKATHON.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0uHqvVEIgXN",
        "outputId": "712f427d-2c40-40b8-cefb-83944c33d8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcbPqHZ2JAWk",
        "outputId": "927b5465-843d-4728-b371-45c05fffca4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 24448, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (41/41), done.\u001b[K\n",
            "remote: Total 24448 (delta 48), reused 23 (delta 23), pack-reused 24384 (from 3)\u001b[K\n",
            "Receiving objects: 100% (24448/24448), 52.91 MiB | 16.39 MiB/s, done.\n",
            "Resolving deltas: 100% (17638/17638), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl9KV_sxJTdk",
        "outputId": "f0b7b5a6-7795-40ad-b9b5-1ed0e634c636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n"
          ]
        }
      ],
      "source": [
        "%cd LLaMA-Factory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2pK1sfW9JWPt",
        "outputId": "cbcfd089-4670-4eb6-8daf-68e6d6f3b214"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ignoring transformers: markers 'sys_platform == \"darwin\"' don't match your environment\n",
            "Collecting transformers!=4.52.0,<=4.52.4,>=4.49.0 (from -r requirements.txt (line 2))\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting datasets<=3.6.0,>=2.16.0 (from -r requirements.txt (line 4))\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting accelerate<=1.7.0,>=1.3.0 (from -r requirements.txt (line 5))\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting peft<=0.15.2,>=0.14.0 (from -r requirements.txt (line 6))\n",
            "  Downloading peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 7))\n",
            "  Downloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting tokenizers<=0.21.1,>=0.19.0 (from -r requirements.txt (line 8))\n",
            "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting gradio<=5.31.0,>=4.38.0 (from -r requirements.txt (line 10))\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (3.10.0)\n",
            "Collecting tyro<0.9.0 (from -r requirements.txt (line 12))\n",
            "  Downloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (0.8.1)\n",
            "Collecting numpy<2.0.0 (from -r requirements.txt (line 15))\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (0.9.0)\n",
            "Collecting modelscope>=1.14.0 (from -r requirements.txt (line 21))\n",
            "  Downloading modelscope-1.28.1-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: hf-transfer in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (0.1.9)\n",
            "Collecting fire (from -r requirements.txt (line 24))\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (2.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (5.29.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (6.0.2)\n",
            "Collecting pydantic<=2.10.6 (from -r requirements.txt (line 29))\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 31)) (0.35.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 32)) (0.116.1)\n",
            "Collecting sse-starlette (from -r requirements.txt (line 33))\n",
            "  Downloading sse_starlette-3.0.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting av (from -r requirements.txt (line 35))\n",
            "  Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 36)) (0.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (0.34.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (2025.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (2.6.0+cu124)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (4.9.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.6.1)\n",
            "Collecting gradio-client==1.10.1 (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10))\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (3.11.1)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (11.3.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.0.20)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.12.5)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (4.14.1)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (15.0.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 11)) (2.9.0.post0)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->-r requirements.txt (line 12)) (0.17.0)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro<0.9.0->-r requirements.txt (line 12)) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro<0.9.0->-r requirements.txt (line 12))\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 16)) (2025.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->-r requirements.txt (line 21)) (75.2.0)\n",
            "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.11/dist-packages (from modelscope>=1.14.0->-r requirements.txt (line 21)) (2.5.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire->-r requirements.txt (line 24)) (3.1.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->-r requirements.txt (line 25)) (4.9.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<=2.10.6->-r requirements.txt (line 29)) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic<=2.10.6->-r requirements.txt (line 29))\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 31)) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn->-r requirements.txt (line 31)) (0.16.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (0.60.0)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->-r requirements.txt (line 36)) (1.1.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (1.3.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (3.12.14)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (1.0.9)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (1.1.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 36)) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->-r requirements.txt (line 36)) (4.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r requirements.txt (line 11)) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers!=4.52.0,<=4.52.4,>=4.49.0->-r requirements.txt (line 2)) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 12)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 12)) (2.19.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 36)) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 36)) (1.17.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (3.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate<=1.7.0,>=1.3.0->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->-r requirements.txt (line 10)) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->-r requirements.txt (line 4)) (1.20.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 36)) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 12)) (0.1.2)\n",
            "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.15.2-py3-none-any.whl (411 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.9.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.8/245.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading modelscope-1.28.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sse_starlette-3.0.2-py3-none-any.whl (11 kB)\n",
            "Downloading av-15.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (39.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=0dcca88754d52aeac446fdb5d09a57dd5e19117e6bfef45be9c43f711216e50c\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: shtab, pydantic-core, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, fire, av, sse-starlette, pydantic, nvidia-cusparse-cu12, nvidia-cudnn-cu12, modelscope, tyro, tokenizers, nvidia-cusolver-cu12, gradio-client, transformers, gradio, datasets, accelerate, trl, peft\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.2\n",
            "    Uninstalling tokenizers-0.21.2:\n",
            "      Successfully uninstalled tokenizers-0.21.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.11.0\n",
            "    Uninstalling gradio_client-1.11.0:\n",
            "      Successfully uninstalled gradio_client-1.11.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.54.0\n",
            "    Uninstalling transformers-4.54.0:\n",
            "      Successfully uninstalled transformers-4.54.0\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.38.2\n",
            "    Uninstalling gradio-5.38.2:\n",
            "      Successfully uninstalled gradio-5.38.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.9.0\n",
            "    Uninstalling accelerate-1.9.0:\n",
            "      Successfully uninstalled accelerate-1.9.0\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.16.0\n",
            "    Uninstalling peft-0.16.0:\n",
            "      Successfully uninstalled peft-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-1.7.0 av-15.0.0 datasets-3.6.0 fire-0.7.0 gradio-5.31.0 gradio-client-1.10.1 modelscope-1.28.1 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.15.2 pydantic-2.10.6 pydantic-core-2.27.2 shtab-1.7.2 sse-starlette-3.0.2 tokenizers-0.21.1 transformers-4.52.4 trl-0.9.6 tyro-0.8.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "57306e326b474843952845399570f9d9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ_4f0iJX8_"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "dcc472aaac0140289b4f470dd38964a2",
            "e9bbab275f684ee182dceecdf5277011",
            "4bb24bd8a0d24772a6207cf605e9909b",
            "083cf2e36441447d99deb5d567d18f1b",
            "7e03c5e317a248d39a971e397528e27e",
            "abc45030ca4644d4a0615958b0feca54",
            "55761d05c5834def9229f81892b22a66",
            "f137da3f2e6449d294a423884d800a3a",
            "3b7de41021de47dc80ce6f2fef5bc17a",
            "b62789985047440d9b499bf2a3afc28a",
            "0c684b512f7b47b1bc73e1184ad3d12c",
            "6c724401b3b44c2c8912be4ff551d012",
            "e4c953e462f345259d4167b96911eaea",
            "9562dbce7d23484ba244b54c2e6bd0e1",
            "152e7442971e44b0b6ce854ec97e996e",
            "03a5e642b3bf4754a4bb65bd77a137c5",
            "53f323933cb148a68a21ad321be5c2a6",
            "fdd514635e884486bf96d82902a01005",
            "3e54b77f60d24ebd82e507896e864a55",
            "edabbd14f8354973ba6d73cbdef3250c"
          ]
        },
        "id": "lyqx5UWbKRZc",
        "outputId": "16d8e467-48c4-4a0e-be69-a60dc6668c1e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcc472aaac0140289b4f470dd38964a2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9h4ixLALp4W",
        "outputId": "b82a62c4-6d11-454e-9ca5-235fd3076d7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mJSyJyjKYY5U",
        "outputId": "f781c822-113c-4663-c2fd-5e3028d9798e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-295d054d-4ebc-441c-808f-bbdd52fffda0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-295d054d-4ebc-441c-808f-bbdd52fffda0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving fine_tune_qa_dataset.json to fine_tune_qa_dataset (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'fine_tune_qa_dataset (1).json': b'[\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb969, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb969 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 5 email accounts. Budget: \\xe2\\x82\\xb969, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb969 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb969, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb969 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 100 bandwidth and priced around \\xe2\\x82\\xb969. Budget: \\xe2\\x82\\xb969, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb969 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb969, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb969 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9159. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 15000 bandwidth and priced around \\xe2\\x82\\xb9159. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 15000 bandwidth and priced around \\xe2\\x82\\xb9159. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9159, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9159 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9199. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9199. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9199. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9199. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9199. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9199. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9199, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Ultimate) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9199 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9149, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9149 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 5 email accounts. Budget: \\xe2\\x82\\xb9149, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9149 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9149, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9149 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 100 bandwidth and priced around \\xe2\\x82\\xb9149. Budget: \\xe2\\x82\\xb9149, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9149 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9149, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9149 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 10000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 10000 email accounts. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 10000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 10000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 100 bandwidth and priced around \\xe2\\x82\\xb9249. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 10000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 10000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9249. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9249, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9249 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 10000 bandwidth and priced around \\xe2\\x82\\xb9299. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a personal website. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1000 website(s) and 1000 email accounts. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 20000 bandwidth and priced around \\xe2\\x82\\xb9299. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9299, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Linux Hosting (Pro) plan. It supports 1000 website(s), offers 1000 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9299 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 75 email accounts. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9479. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 75 email accounts. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9479. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9479, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9479 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9599, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9599 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 100 email accounts. Budget: \\xe2\\x82\\xb9599, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9599 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9599, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9599 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9599. Budget: \\xe2\\x82\\xb9599, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9599 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9599, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9599 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 150 email accounts. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9839. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 150 email accounts. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9839. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9839, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9839 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 75 email accounts. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9539. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 75 email accounts. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9539. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9539, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Starter SSD) plan. It supports 1 website(s), offers 75 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9539 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 100 email accounts. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9659. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 100 email accounts. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9659. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9659, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Advanced SSD) plan. It supports 1 website(s), offers 100 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9659 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 150 email accounts. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: No. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9899. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a business website. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 1 website(s) and 150 email accounts. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: Yes, Domain: Yes. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 50000 bandwidth and priced around \\xe2\\x82\\xb9899. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9899, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock Cloud Hosting (Business SSD) plan. It supports 1 website(s), offers 150 email accounts, comes with SSL (Free), and costs \\xe2\\x82\\xb9899 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a ecommerce website. Budget: \\xe2\\x82\\xb9399, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9399 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 0 website(s) and 0 email accounts. Budget: \\xe2\\x82\\xb9399, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9399 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: No, Domain: No. Budget: \\xe2\\x82\\xb9399, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9399 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 500 bandwidth and priced around \\xe2\\x82\\xb9399. Budget: \\xe2\\x82\\xb9399, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9399 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9399, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9399 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a ecommerce website. Budget: \\xe2\\x82\\xb9799, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 4) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9799 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 0 website(s) and 0 email accounts. Budget: \\xe2\\x82\\xb9799, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 4) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9799 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: No, Domain: No. Budget: \\xe2\\x82\\xb9799, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 4) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9799 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 500 bandwidth and priced around \\xe2\\x82\\xb9799. Budget: \\xe2\\x82\\xb9799, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 4) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9799 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9799, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 4) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9799 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a ecommerce website. Budget: \\xe2\\x82\\xb9289, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 1) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9289 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 0 website(s) and 0 email accounts. Budget: \\xe2\\x82\\xb9289, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 1) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9289 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: No, Domain: No. Budget: \\xe2\\x82\\xb9289, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 1) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9289 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 500 bandwidth and priced around \\xe2\\x82\\xb9289. Budget: \\xe2\\x82\\xb9289, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 1) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9289 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in IN with free SSL. Budget: \\xe2\\x82\\xb9289, Region: IN.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 1) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9289 per month. Hosting location is IN.\"\\n  },\\n  {\\n    \"instruction\": \"I want a hosting plan for a ecommerce website. Budget: \\xe2\\x82\\xb9349, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9349 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"I\\'m looking for a plan that supports 0 website(s) and 0 email accounts. Budget: \\xe2\\x82\\xb9349, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9349 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Suggest a hosting plan with SSL: No, Domain: No. Budget: \\xe2\\x82\\xb9349, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9349 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Need a plan with at least 500 bandwidth and priced around \\xe2\\x82\\xb9349. Budget: \\xe2\\x82\\xb9349, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9349 per month. Hosting location is US.\"\\n  },\\n  {\\n    \"instruction\": \"Looking for a plan hosted in US with free SSL. Budget: \\xe2\\x82\\xb9349, Region: US.\",\\n    \"input\": \"\",\\n    \"output\": \"Based on your needs, you can choose the BigRock VPS Hosting (NVMe 2) plan. It supports 0 website(s), offers 0 email accounts, comes with SSL (), and costs \\xe2\\x82\\xb9349 per month. Hosting location is US.\"\\n  }\\n]'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGvkudilVfMy",
        "outputId": "b7f4237c-d922-46b4-8996-a5479e52aaee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Fixed DATA_CONFIG path in constants.py\n"
          ]
        }
      ],
      "source": [
        "const_path = \"src/llamafactory/extras/constants.py\"\n",
        "\n",
        "with open(const_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open(const_path, \"w\") as f:\n",
        "    for line in lines:\n",
        "        if line.strip().startswith(\"DATA_CONFIG\"):\n",
        "            f.write('DATA_CONFIG = \"src/llamafactory/data/dataset_info.json\"\\n')\n",
        "        else:\n",
        "            f.write(line)\n",
        "\n",
        "print(\"✅ Fixed DATA_CONFIG path in constants.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiyfoUZEVLR5",
        "outputId": "e07c556e-79f5-4cb2-94c1-6603820080ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔎 Searching for: 'DATA_CONFIG' in src/llamafactory/extras/constants.py\n",
            "\n",
            "Line 38: DATA_CONFIG = \"src/llamafactory/data/dataset_info.json\"\n"
          ]
        }
      ],
      "source": [
        "def search_in_file(file_path, search_string):\n",
        "    print(f\"🔎 Searching for: '{search_string}' in {file_path}\\n\")\n",
        "    found = False\n",
        "    with open(file_path, \"r\") as f:\n",
        "        for i, line in enumerate(f, 1):\n",
        "            if search_string in line:\n",
        "                print(f\"Line {i}: {line.strip()}\")\n",
        "                found = True\n",
        "    if not found:\n",
        "        print(\"❌ String not found.\")\n",
        "\n",
        "# Example usage:\n",
        "search_in_file(\"src/llamafactory/extras/constants.py\", \"DATA_CONFIG\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCR1_2aiRCzs",
        "outputId": "2b60f454-cc16-4109-a350-1723728e712c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ dataset_info.json created and registered!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "dataset_info_path = \"src/llamafactory/data/dataset_info.json\"\n",
        "os.makedirs(os.path.dirname(dataset_info_path), exist_ok=True)\n",
        "\n",
        "data = {\n",
        "    \"fine_tune_qa_dataset.json\": {\n",
        "        \"file_name\": \"fine_tune_qa_dataset.json\",\n",
        "        \"path\": \"fine_tune_qa_dataset.json\",\n",
        "        \"type\": \"alpaca\"\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(dataset_info_path, \"w\") as f:\n",
        "    json.dump(data, f, indent=2)\n",
        "\n",
        "print(\"✅ dataset_info.json created and registered!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4YKFvvgTSQh",
        "outputId": "2d4f4472-9175-4d96-ace0-acac5f503796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"fine_tune_qa_dataset.json\": {\n",
            "    \"file_name\": \"fine_tune_qa_dataset.json\",\n",
            "    \"path\": \"fine_tune_qa_dataset.json\",\n",
            "    \"type\": \"alpaca\"\n",
            "  }\n",
            "}"
          ]
        }
      ],
      "source": [
        "!cat src/llamafactory/data/dataset_info.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDQ9ynR6Tz6A",
        "outputId": "c40a691b-606a-4efd-d1c7-cde09ce2e717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_CONFIG path: src/llamafactory/data/dataset_info.json\n",
            "{\n",
            "  \"fine_tune_qa_dataset.json\": {\n",
            "    \"file_name\": \"fine_tune_qa_dataset.json\",\n",
            "    \"path\": \"fine_tune_qa_dataset.json\",\n",
            "    \"type\": \"alpaca\"\n",
            "  }\n",
            "}"
          ]
        }
      ],
      "source": [
        "from src.llamafactory.data.parser import DATA_CONFIG\n",
        "print(\"DATA_CONFIG path:\", DATA_CONFIG)\n",
        "!cat $DATA_CONFIG\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python src/train.py \\\n",
        "  --stage sft \\\n",
        "  --do_train \\\n",
        "  --model_name_or_path mistralai/Mistral-7B-v0.1 \\\n",
        "  --dataset cleaned_dataset.json \\\n",
        "  --template mistral \\\n",
        "  --finetuning_type lora \\\n",
        "  --output_dir ./lora-out \\\n",
        "  --cutoff_len 1024 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50 \\\n",
        "  --per_device_train_batch_size 4 \\\n",
        "  --gradient_accumulation_steps 2 \\\n",
        "  --lr_scheduler_type cosine \\\n",
        "  --learning_rate 5e-5 \\\n",
        "  --num_train_epochs 1 \\\n",
        "  --plot_loss \\\n",
        "  --fp16\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UIhWAUN3eOp",
        "outputId": "b07c0949-c9b2-42ac-8c79-133f60369b7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-02 17:39:28.637137: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754156368.671950   13059 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754156368.683426   13059 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-08-02 17:39:33] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:34,494 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:34,494 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:34,494 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:34,494 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:34,494 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:34,494 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:698] 2025-08-02 17:39:36,043 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-08-02 17:39:36,046 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": null,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:36,537 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:36,537 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:36,537 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:36,537 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:36,537 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:39:36,537 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-08-02 17:39:36] llamafactory.data.template:143 >> Add pad token: </s>\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/parser.py\", line 107, in get_dataset_list\n",
            "    with open(config_path) as f:\n",
            "         ^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/src/llamafactory/data/dataset_info.json'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/LLaMA-Factory/src/train.py\", line 28, in <module>\n",
            "    main()\n",
            "  File \"/content/LLaMA-Factory/src/train.py\", line 19, in main\n",
            "    run_exp()\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 110, in run_exp\n",
            "    _training_function(config={\"args\": args, \"callbacks\": callbacks})\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/tuner.py\", line 72, in _training_function\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/train/sft/workflow.py\", line 51, in run_sft\n",
            "    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 304, in get_dataset\n",
            "    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/loader.py\", line 178, in _get_merged_dataset\n",
            "    for dataset_name, dataset_attr in zip(dataset_names, get_dataset_list(dataset_names, data_args.dataset_dir)):\n",
            "                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/LLaMA-Factory/src/llamafactory/data/parser.py\", line 111, in get_dataset_list\n",
            "    raise ValueError(f\"Cannot open {config_path} due to {str(err)}.\")\n",
            "ValueError: Cannot open data/src/llamafactory/data/dataset_info.json due to [Errno 2] No such file or directory: 'data/src/llamafactory/data/dataset_info.json'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train.py \\\n",
        "  --stage sft \\\n",
        "  --model_name_or_path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n",
        "  --do_train \\\n",
        "  --dataset_dir ./ \\\n",
        "  --dataset fine_tune_qa_dataset.json \\\n",
        "  --template default \\\n",
        "  --finetuning_type lora \\\n",
        "  --output_dir ./tinyllama-output \\\n",
        "  --cutoff_len 512 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 100 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --per_device_train_batch_size 2 \\\n",
        "  --gradient_accumulation_steps 4 \\\n",
        "  --learning_rate 2e-4 \\\n",
        "  --bf16\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpTHtaAw-fiO",
        "outputId": "95dc56d8-64d1-4fca-c4f5-ecc29b314e20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-02 19:00:54.324695: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754161254.344654   32437 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754161254.350999   32437 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-08-02 19:00:59] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.bfloat16\n",
            "tokenizer_config.json: 1.29kB [00:00, 2.30MB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:00<00:00, 28.1MB/s]\n",
            "tokenizer.json: 1.84MB [00:00, 13.5MB/s]\n",
            "special_tokens_map.json: 100% 551/551 [00:00<00:00, 3.68MB/s]\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:03,462 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:03,463 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:03,463 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:03,463 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:03,463 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:03,463 >> loading file chat_template.jinja from cache at None\n",
            "config.json: 100% 608/608 [00:00<00:00, 4.32MB/s]\n",
            "[INFO|configuration_utils.py:698] 2025-08-02 19:01:05,442 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-08-02 19:01:05,446 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:05,924 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:05,924 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:05,924 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:05,924 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:05,925 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 19:01:05,925 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|2025-08-02 19:01:06] llamafactory.data.loader:143 >> Loading dataset fine_tune_qa_dataset.json...\n",
            "Converting format of dataset: 100% 150/150 [00:00<00:00, 8988.31 examples/s]\n",
            "Running tokenizer on dataset: 100% 150/150 [00:00<00:00, 2079.33 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[12968, 29901, 306, 864, 263, 23376, 3814, 363, 263, 7333, 4700, 29889, 7038, 657, 29901, 29871, 30620, 29953, 29929, 29892, 11069, 29901, 3148, 29889, 2, 29871, 13, 7900, 22137, 29901, 16564, 373, 596, 4225, 29892, 366, 508, 6755, 278, 7997, 29934, 1698, 8074, 16956, 292, 313, 855, 4254, 29897, 3814, 29889, 739, 11286, 29871, 29896, 4700, 29898, 29879, 511, 16688, 29871, 29945, 4876, 15303, 29892, 5304, 411, 17122, 313, 20475, 511, 322, 21544, 29871, 30620, 29953, 29929, 639, 4098, 29889, 16956, 292, 4423, 338, 3148, 29889, 2, 29871, 13]\n",
            "inputs:\n",
            "Human: I want a hosting plan for a personal website. Budget: ₹69, Region: US.</s> \n",
            "Assistant: Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs ₹69 per month. Hosting location is US.</s> \n",
            "\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16564, 373, 596, 4225, 29892, 366, 508, 6755, 278, 7997, 29934, 1698, 8074, 16956, 292, 313, 855, 4254, 29897, 3814, 29889, 739, 11286, 29871, 29896, 4700, 29898, 29879, 511, 16688, 29871, 29945, 4876, 15303, 29892, 5304, 411, 17122, 313, 20475, 511, 322, 21544, 29871, 30620, 29953, 29929, 639, 4098, 29889, 16956, 292, 4423, 338, 3148, 29889, 2, 29871, 13]\n",
            "labels:\n",
            "Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs ₹69 per month. Hosting location is US.</s> \n",
            "\n",
            "[INFO|configuration_utils.py:698] 2025-08-02 19:01:07,063 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-08-02 19:01:07,064 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|2025-08-02 19:01:07] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "model.safetensors: 100% 2.20G/2.20G [00:10<00:00, 203MB/s]\n",
            "[INFO|modeling_utils.py:1151] 2025-08-02 19:01:18,903 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/model.safetensors\n",
            "[INFO|modeling_utils.py:2241] 2025-08-02 19:01:18,903 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
            "[INFO|configuration_utils.py:1135] 2025-08-02 19:01:18,905 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"use_cache\": false\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:5131] 2025-08-02 19:01:19,791 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:5139] 2025-08-02 19:01:19,791 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 922kB/s]\n",
            "[INFO|configuration_utils.py:1090] 2025-08-02 19:01:20,743 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/generation_config.json\n",
            "[INFO|configuration_utils.py:1135] 2025-08-02 19:01:20,743 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"max_length\": 2048,\n",
            "  \"pad_token_id\": 0\n",
            "}\n",
            "\n",
            "[INFO|2025-08-02 19:01:20] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
            "[INFO|2025-08-02 19:01:20] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
            "[INFO|2025-08-02 19:01:20] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
            "[INFO|2025-08-02 19:01:20] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
            "[INFO|2025-08-02 19:01:20] llamafactory.model.model_utils.misc:143 >> Found linear modules: k_proj,o_proj,up_proj,down_proj,gate_proj,v_proj,q_proj\n",
            "[INFO|2025-08-02 19:01:22] llamafactory.model.loader:143 >> trainable params: 6,307,840 || all params: 1,106,356,224 || trainable%: 0.5701\n",
            "[INFO|trainer.py:756] 2025-08-02 19:01:22,306 >> Using auto half precision backend\n",
            "[INFO|trainer.py:2409] 2025-08-02 19:01:22,732 >> ***** Running training *****\n",
            "[INFO|trainer.py:2410] 2025-08-02 19:01:22,732 >>   Num examples = 150\n",
            "[INFO|trainer.py:2411] 2025-08-02 19:01:22,732 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2412] 2025-08-02 19:01:22,732 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:2415] 2025-08-02 19:01:22,732 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:2416] 2025-08-02 19:01:22,732 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:2417] 2025-08-02 19:01:22,732 >>   Total optimization steps = 57\n",
            "[INFO|trainer.py:2418] 2025-08-02 19:01:22,735 >>   Number of trainable parameters = 6,307,840\n",
            "[INFO|integration_utils.py:832] 2025-08-02 19:01:22,741 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Create a W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Create an account here: https://wandb.ai/authorize?signup=true&ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msumathivarma3399\u001b[0m (\u001b[33msumathivarma3399-newfold-digital\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/LLaMA-Factory/wandb/run-20250802_190342-9wz8gwvc\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./tinyllama-output\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/sumathivarma3399-newfold-digital/llamafactory\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/sumathivarma3399-newfold-digital/llamafactory/runs/9wz8gwvc\u001b[0m\n",
            "{'loss': 1.138, 'grad_norm': 1.5338444709777832, 'learning_rate': 0.00016842105263157895, 'epoch': 0.53}\n",
            "{'loss': 0.1835, 'grad_norm': 2.0370371341705322, 'learning_rate': 0.00013333333333333334, 'epoch': 1.05}\n",
            "{'loss': 0.0956, 'grad_norm': 0.7260948419570923, 'learning_rate': 9.824561403508771e-05, 'epoch': 1.59}\n",
            "{'loss': 0.0762, 'grad_norm': 0.4564022719860077, 'learning_rate': 6.31578947368421e-05, 'epoch': 2.11}\n",
            "{'loss': 0.0578, 'grad_norm': 0.7987633347511292, 'learning_rate': 2.8070175438596492e-05, 'epoch': 2.64}\n",
            "100% 57/57 [03:06<00:00,  3.04s/it][INFO|trainer.py:3993] 2025-08-02 19:06:50,844 >> Saving model checkpoint to ./tinyllama-output/checkpoint-57\n",
            "[INFO|configuration_utils.py:698] 2025-08-02 19:06:51,368 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-08-02 19:06:51,370 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-08-02 19:06:51,478 >> chat template saved in ./tinyllama-output/checkpoint-57/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-08-02 19:06:51,479 >> tokenizer config file saved in ./tinyllama-output/checkpoint-57/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-08-02 19:06:51,479 >> Special tokens file saved in ./tinyllama-output/checkpoint-57/special_tokens_map.json\n",
            "[INFO|trainer.py:2676] 2025-08-02 19:06:51,724 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 328.99, 'train_samples_per_second': 1.368, 'train_steps_per_second': 0.173, 'train_loss': 0.2788019833857553, 'epoch': 3.0}\n",
            "100% 57/57 [03:07<00:00,  3.28s/it]\n",
            "[INFO|trainer.py:3993] 2025-08-02 19:06:51,731 >> Saving model checkpoint to ./tinyllama-output\n",
            "[INFO|configuration_utils.py:698] 2025-08-02 19:06:52,253 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--TinyLlama--TinyLlama-1.1B-Chat-v1.0/snapshots/fe8a4ea1ffedaf415f4da2f062534de366a451e6/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-08-02 19:06:52,255 >> Model config LlamaConfig {\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": 64,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 2048,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 5632,\n",
            "  \"max_position_embeddings\": 2048,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 22,\n",
            "  \"num_key_value_heads\": 4,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": null,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2356] 2025-08-02 19:06:52,350 >> chat template saved in ./tinyllama-output/chat_template.jinja\n",
            "[INFO|tokenization_utils_base.py:2525] 2025-08-02 19:06:52,351 >> tokenizer config file saved in ./tinyllama-output/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2534] 2025-08-02 19:06:52,352 >> Special tokens file saved in ./tinyllama-output/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               =   285962GF\n",
            "  train_loss               =     0.2788\n",
            "  train_runtime            = 0:05:28.99\n",
            "  train_samples_per_second =      1.368\n",
            "  train_steps_per_second   =      0.173\n",
            "[INFO|modelcard.py:450] 2025-08-02 19:06:52,414 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[1;34mwandb\u001b[0m: \n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33m./tinyllama-output\u001b[0m at: \u001b[34mhttps://wandb.ai/sumathivarma3399-newfold-digital/llamafactory/runs/9wz8gwvc\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250802_190342-9wz8gwvc/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./tinyllama-output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNxbwyR4MDj9",
        "outputId": "4d8b2044-ba7a-4a1f-f8da-41ed05853f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "adapter_config.json\t   README.md\t\t    tokenizer.model\n",
            "adapter_model.safetensors  runs\t\t\t    trainer_log.jsonl\n",
            "all_results.json\t   special_tokens_map.json  trainer_state.json\n",
            "chat_template.jinja\t   tokenizer_config.json    training_args.bin\n",
            "checkpoint-57\t\t   tokenizer.json\t    train_results.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "adapter_path = \"./tinyllama-output\"\n",
        "\n",
        "# Load base model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
        "\n",
        "# Load LoRA adapter\n",
        "model = PeftModel.from_pretrained(model, adapter_path)\n",
        "\n",
        "# Run inference\n",
        "prompt = \"### Instruction:\\nSuggest a hosting plan for a small blog with SSL and budget ₹349, Region: US.\\n\\n### Response:\\n\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=200)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "\n",
        "\n",
        "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znXenHigMN5M",
        "outputId": "7771efb8-4a79-4ced-b81a-836b581f60c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Instruction:\n",
            "Suggest a hosting plan for a small blog with SSL and budget ₹349, Region: US.\n",
            "\n",
            "### Response:\n",
            "Based on your needs, you can choose the BigRock Linux Hosting (Advanced) plan. It supports 1 website(s), offers 1000 email accounts, comes with SSL (Free), and costs ₹349 per month. Hosting location is US.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "from types import SimpleNamespace\n",
        "\n",
        "args = SimpleNamespace(\n",
        "    base_model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    lora_model=\"./tinyllama-output\",\n",
        "    output_dir=\"./merged-tinyllama\"\n",
        ")\n",
        "\n",
        "# Load and merge\n",
        "base = AutoModelForCausalLM.from_pretrained(args.base_model, torch_dtype=torch.float16)\n",
        "model = PeftModel.from_pretrained(base, args.lora_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Save merged model\n",
        "model.save_pretrained(args.output_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(args.base_model)\n",
        "tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "print(f\"✅ Merged model saved to: {args.output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ86yVyPQJFB",
        "outputId": "a6f2556e-eb8e-4245-e06c-cbe17ad6ed7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Merged model saved to: ./merged-tinyllama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r merged-tinyllama.zip merged-tinyllama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUMmalHOQ6cx",
        "outputId": "0ecfd0cf-0635-4014-db25-226fb62354c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: merged-tinyllama/ (stored 0%)\n",
            "  adding: merged-tinyllama/tokenizer.model (deflated 55%)\n",
            "  adding: merged-tinyllama/generation_config.json (deflated 29%)\n",
            "  adding: merged-tinyllama/model.safetensors (deflated 10%)\n",
            "  adding: merged-tinyllama/special_tokens_map.json (deflated 79%)\n",
            "  adding: merged-tinyllama/tokenizer.json (deflated 85%)\n",
            "  adding: merged-tinyllama/config.json (deflated 48%)\n",
            "  adding: merged-tinyllama/tokenizer_config.json (deflated 69%)\n",
            "  adding: merged-tinyllama/chat_template.jinja (deflated 60%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"merged-tinyllama.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "Zy9jgwxkRlzZ",
        "outputId": "010970cf-8b52-4de4-91a6-fc620c51237b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "Cannot find file: merged-tinyllama.zip",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2415696946.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merged-tinyllama.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: merged-tinyllama.zip"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/llamafactory/utils/merge_lora.py \\\n",
        "  --base_model_name_or_path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n",
        "  --adapter_name_or_path ./tinyllama-output \\\n",
        "  --output_dir ./merged-tinyllama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXYPgrjQO5sd",
        "outputId": "ffcffe93-01c8-4573-f793-f971a9ef4238"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/LLaMA-Factory/src/llamafactory/utils/merge_lora.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJczY5N9QovL",
        "outputId": "aa723ec9-16e7-47dc-c768-03f94a6d9e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-02 17:58:10.108489: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754157490.127946   17786 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754157490.134230   17786 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "[INFO|2025-08-02 17:58:15] llamafactory.hparams.parser:410 >> Process rank: 0, world size: 1, device: cuda:0, distributed training: False, compute dtype: torch.float16\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:15,590 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:15,590 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:15,590 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:15,590 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:15,590 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:15,590 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:698] 2025-08-02 17:58:17,109 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-08-02 17:58:17,112 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": null,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:17,598 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:17,598 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:17,598 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:17,598 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:17,598 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2023] 2025-08-02 17:58:17,598 >> loading file chat_template.jinja from cache at None\n",
            "[WARNING|2025-08-02 17:58:17] llamafactory.data.template:148 >> `template` was not specified, use `empty` template.\n",
            "[INFO|2025-08-02 17:58:17] llamafactory.data.template:143 >> Add pad token: </s>\n",
            "[INFO|2025-08-02 17:58:17] llamafactory.data.loader:143 >> Loading dataset fine_tune_qa_dataset.json...\n",
            "Converting format of dataset: 100% 150/150 [00:00<00:00, 9231.10 examples/s]\n",
            "Running tokenizer on dataset: 100% 150/150 [00:00<00:00, 2591.04 examples/s]\n",
            "training example:\n",
            "input_ids:\n",
            "[315, 947, 264, 19117, 2623, 354, 264, 3327, 4400, 28723, 7095, 527, 28747, 28705, 29652, 28784, 28774, 28725, 13365, 28747, 2223, 28723, 17158, 356, 574, 3208, 28725, 368, 541, 4987, 272, 6375, 28754, 629, 19486, 15511, 288, 325, 718, 4136, 28731, 2623, 28723, 661, 11562, 28705, 28740, 4400, 28732, 28713, 557, 5751, 28705, 28782, 4927, 9855, 28725, 3435, 395, 20669, 325, 10998, 557, 304, 6966, 28705, 29652, 28784, 28774, 660, 2102, 28723, 15511, 288, 4723, 349, 2223, 28723]\n",
            "inputs:\n",
            "I want a hosting plan for a personal website. Budget: ₹69, Region: US. Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs ₹69 per month. Hosting location is US.\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 17158, 356, 574, 3208, 28725, 368, 541, 4987, 272, 6375, 28754, 629, 19486, 15511, 288, 325, 718, 4136, 28731, 2623, 28723, 661, 11562, 28705, 28740, 4400, 28732, 28713, 557, 5751, 28705, 28782, 4927, 9855, 28725, 3435, 395, 20669, 325, 10998, 557, 304, 6966, 28705, 29652, 28784, 28774, 660, 2102, 28723, 15511, 288, 4723, 349, 2223, 28723]\n",
            "labels:\n",
            "Based on your needs, you can choose the BigRock Linux Hosting (Starter) plan. It supports 1 website(s), offers 5 email accounts, comes with SSL (Free), and costs ₹69 per month. Hosting location is US.\n",
            "[INFO|configuration_utils.py:698] 2025-08-02 17:58:18,697 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/config.json\n",
            "[INFO|configuration_utils.py:770] 2025-08-02 17:58:18,698 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"head_dim\": null,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.52.4\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|2025-08-02 17:58:18] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 66.8MB/s]\n",
            "[INFO|modeling_utils.py:1151] 2025-08-02 17:58:20,090 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-v0.1/snapshots/27d67f1b5f57dc0953326b2601d68371d40ea8da/model.safetensors.index.json\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 43.1k/9.94G [00:02<131:36:09, 21.0kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 203k/4.54G [00:02<12:59:44, 97.0kB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 7.40M/4.54G [00:03<33:56, 2.23MB/s]  \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 9.69M/9.94G [00:04<1:09:34, 2.38MB/s]  \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 15.9M/4.54G [00:04<18:21, 4.11MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 15.1M/9.94G [00:05<47:01, 3.52MB/s]  \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 23.8M/9.94G [00:06<34:38, 4.77MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 54.2M/9.94G [00:12<31:41, 5.20MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 33.3M/4.54G [00:12<28:19, 2.65MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 50.1M/4.54G [00:14<18:37, 4.02MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 105M/9.94G [00:15<18:54, 8.67MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 66.0M/4.54G [00:22<25:00, 2.98MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 107M/4.54G [00:24<12:50, 5.76MB/s] \u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 164M/9.94G [00:25<22:57, 7.10MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 131M/4.54G [00:25<10:05, 7.28MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 174M/4.54G [00:26<05:44, 12.7MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 223M/4.54G [00:34<08:05, 8.89MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 326M/4.54G [00:36<04:32, 15.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 164M/9.94G [00:37<22:57, 7.10MB/s]\u001b[A\u001b[A"
          ]
        }
      ],
      "source": [
        "!python src/train.py \\\n",
        "  --stage sft \\\n",
        "  --do_train \\\n",
        "  --dataset fine_tune_qa_dataset.json \\\n",
        "  --dataset_dir ./ \\\n",
        "  --model_name_or_path mistralai/Mistral-7B-v0.1 \\\n",
        "  --output_dir saved/mistral-sft \\\n",
        "  --overwrite_output_dir \\\n",
        "  --fp16 \\\n",
        "  --per_device_train_batch_size 1 \\\n",
        "  --logging_steps 10 \\\n",
        "  --save_steps 50 \\\n",
        "  --num_train_epochs 1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNqhTiPDMFsJRAGNsrEOafI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dcc472aaac0140289b4f470dd38964a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_55761d05c5834def9229f81892b22a66"
          }
        },
        "e9bbab275f684ee182dceecdf5277011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f137da3f2e6449d294a423884d800a3a",
            "placeholder": "​",
            "style": "IPY_MODEL_3b7de41021de47dc80ce6f2fef5bc17a",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "4bb24bd8a0d24772a6207cf605e9909b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_b62789985047440d9b499bf2a3afc28a",
            "placeholder": "​",
            "style": "IPY_MODEL_0c684b512f7b47b1bc73e1184ad3d12c",
            "value": ""
          }
        },
        "083cf2e36441447d99deb5d567d18f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_6c724401b3b44c2c8912be4ff551d012",
            "style": "IPY_MODEL_e4c953e462f345259d4167b96911eaea",
            "value": true
          }
        },
        "7e03c5e317a248d39a971e397528e27e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9562dbce7d23484ba244b54c2e6bd0e1",
            "style": "IPY_MODEL_152e7442971e44b0b6ce854ec97e996e",
            "tooltip": ""
          }
        },
        "abc45030ca4644d4a0615958b0feca54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03a5e642b3bf4754a4bb65bd77a137c5",
            "placeholder": "​",
            "style": "IPY_MODEL_53f323933cb148a68a21ad321be5c2a6",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "55761d05c5834def9229f81892b22a66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "f137da3f2e6449d294a423884d800a3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b7de41021de47dc80ce6f2fef5bc17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b62789985047440d9b499bf2a3afc28a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c684b512f7b47b1bc73e1184ad3d12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c724401b3b44c2c8912be4ff551d012": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4c953e462f345259d4167b96911eaea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9562dbce7d23484ba244b54c2e6bd0e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "152e7442971e44b0b6ce854ec97e996e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "03a5e642b3bf4754a4bb65bd77a137c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53f323933cb148a68a21ad321be5c2a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd514635e884486bf96d82902a01005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e54b77f60d24ebd82e507896e864a55",
            "placeholder": "​",
            "style": "IPY_MODEL_edabbd14f8354973ba6d73cbdef3250c",
            "value": "Connecting..."
          }
        },
        "3e54b77f60d24ebd82e507896e864a55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "edabbd14f8354973ba6d73cbdef3250c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}